{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Some imports",
   "id": "48283fa4698c7758"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.datasets import mnist\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### An existing k-means algorithm with three different distance metrics",
   "id": "5cd01a369c16e191"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
    "\n",
    "class KMeansWithCustomDistance(KMeans):\n",
    "    def __init__(self, n_clusters, init, distance='euclidean', **kwargs):\n",
    "        super().__init__(n_clusters=n_clusters, init=init, **kwargs)\n",
    "        self.distance = distance\n",
    "\n",
    "    def _pairwise_distances_argmin(self, X):\n",
    "        if self.distance == 'euclidean':\n",
    "            return np.argmin(euclidean_distances(X, self.cluster_centers_), axis=1)\n",
    "        elif self.distance == 'manhattan':\n",
    "            return np.argmin(manhattan_distances(X, self.cluster_centers_), axis=1)\n",
    "        elif self.distance == 'cosine':\n",
    "            return np.argmin(1 - cosine_distances(X, self.cluster_centers_), axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid distance metric: {self.distance}\")"
   ],
   "id": "61d9618ff431230e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get MNIST data",
   "id": "ad564f7a2e8c53cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # load_data() function takes 60000 for training and 10000 for test. \n",
    "X_train = X_train.astype('float32') / 255.0 # normalize\n",
    "X_train = X_train.reshape(60000, 28*28) # to dimension reducing with 784=28*28\n",
    "\n",
    "# According to HW4, we need 56000 data, because get %80 training data, %20 test data, so 70.000 * 0.8 = 56.000, \n",
    "X_train = X_train[:5600] # for now, 5600 > %10\n",
    "y_train = y_train[:5600]\n",
    "X_test = X_test[:5600]\n",
    "y_test = y_test[:5600]\n",
    "kmeans = KMeansWithCustomDistance(n_clusters=10, init='k-means++', distance='manhattan')"
   ],
   "id": "f22c1b9f09b75d0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get clusters",
   "id": "abf42a91a4397eb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kmeans.fit(X_train)\n",
    "labels_manhattan = kmeans.predict(X_train)\n",
    "print(\"Cluster labels (Manhattan):\", labels_manhattan)\n",
    "\n",
    "centers_manhattan = kmeans.cluster_centers_\n",
    "print(\"Cluster centers (Manhattan):\", centers_manhattan)\n"
   ],
   "id": "60daca2f0409ad72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### If you see the diagram, you will uncommit",
   "id": "fdce9e209cd741a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# \n",
    "# kmeans.fit(X_train_pca)\n",
    "# labels_manhattan_pca = kmeans.predict(X_train_pca)\n",
    "# \n",
    "# centers_pca_manhattan = kmeans.cluster_centers_\n",
    "# print(centers_pca_manhattan)\n",
    "# \n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=labels_manhattan_pca, cmap='viridis')\n",
    "# plt.scatter(centers_pca_manhattan[:, 0], centers_pca_manhattan[:, 1], c='red', s=100, alpha=0.5)\n",
    "# plt.title('Cluster Centers and Data Points (PCA)')\n",
    "# plt.show()"
   ],
   "id": "71afaf725bd1ea8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## P.S.\n",
    "But these are not real label of each image, since the output of the kmeans.labels_ is just group id for clustering. For example, 6 in kmeans.labels_ has similar features with another 6 in kmeans.labels_. There is no more meaning from the label.\n",
    "\n",
    "To match it with real label, we can tackle the follow things:\n",
    "\n",
    "- Combine each images in the same group\n",
    "- Check Frequency distribution of actual labels (using np.bincount)\n",
    "- Find the Maximum frequent label (through np.argmax), and set the label."
   ],
   "id": "1b40a2395cf12a95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def infer_cluster_labels(kmeans, actual_labels):\n",
    "    \"\"\"\n",
    "    Associates most probable label with each cluster in KMeans model\n",
    "    returns: dictionary of clusters assigned to each label\n",
    "    \"\"\"\n",
    "\n",
    "    inferred_labels = {}\n",
    "\n",
    "    # Loop through the clusters\n",
    "    for i in range(kmeans.n_clusters):\n",
    "\n",
    "        # find index of points in cluster\n",
    "        labels = []\n",
    "        index = np.where(kmeans.labels_ == i)\n",
    "\n",
    "        # append actual labels for each point in cluster\n",
    "        labels.append(actual_labels[index])\n",
    "\n",
    "        # determine most common label\n",
    "        if len(labels[0]) == 1:\n",
    "            counts = np.bincount(labels[0])\n",
    "        else:\n",
    "            counts = np.bincount(np.squeeze(labels))\n",
    "\n",
    "        # assign the cluster to a value in the inferred_labels dictionary\n",
    "        if np.argmax(counts) in inferred_labels:\n",
    "            # append the new number to the existing array at this slot\n",
    "            inferred_labels[np.argmax(counts)].append(i)\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "            inferred_labels[np.argmax(counts)] = [i]\n",
    "        \n",
    "    return inferred_labels  \n",
    "\n",
    "def infer_data_labels(X_labels, cluster_labels):\n",
    "    \"\"\"\n",
    "    Determines label for each array, depending on the cluster it has been assigned to.\n",
    "    returns: predicted labels for each array\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty array of len(X)\n",
    "    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)\n",
    "    \n",
    "    for i, cluster in enumerate(X_labels):\n",
    "        for key, value in cluster_labels.items():\n",
    "            if cluster in value:\n",
    "                predicted_labels[i] = key\n",
    "                \n",
    "    return predicted_labels"
   ],
   "id": "f2a9aad4a140194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kmeans.fit(X_train)\n",
    "cluster_labels = infer_cluster_labels(kmeans, y_train)\n",
    "X_clusters_labels = kmeans.predict(X_train)\n",
    "predicted_labels_with_argmax = infer_data_labels(X_clusters_labels, cluster_labels)\n",
    "\n",
    "print(predicted_labels_with_argmax[:20])\n",
    "print(y_train[:20])"
   ],
   "id": "de291ef210ff8182",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### If you see Kmeans performance for different cluster, you will uncommit",
   "id": "7c95dca26328a9f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# \n",
    "# clusters = [10, 16, 36, 64, 144, 256]\n",
    "# acc_list = []\n",
    "# \n",
    "# for n_clusters in clusters:\n",
    "#     estimator = KMeansWithCustomDistance(n_clusters=n_clusters, init='k-means++', distance='manhattan')\n",
    "#     estimator.fit(X_train)\n",
    "# \n",
    "#     # Determine predicted labels\n",
    "#     cluster_labels = infer_cluster_labels(estimator, y_train)\n",
    "#     prediction = infer_data_labels(estimator.labels_, cluster_labels)\n",
    "#     \n",
    "#     acc = accuracy_score(y_train, prediction)\n",
    "#     acc_list.append(acc)\n",
    "#     print('Accuracy: {}\\n'.format(acc))"
   ],
   "id": "e28ff305ed410b76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', 7)\n",
    "\n",
    "row_labels = [f'Label{i}' for i in range(10)]\n",
    "column_labels = [f'Cluster{i+1}' for i in range(10)]\n",
    "\n",
    "accuracy_scores_for_trainin_error = []\n",
    "confusion_matrices_for_trainin_error = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation this function gives me an index subset of X_train\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    kmeans.fit(X_train_fold)\n",
    "    \n",
    "    cluster_labels_fold = infer_cluster_labels(kmeans, y_train_fold)\n",
    "    X_cluster_labels_fold = kmeans.predict(X_train_fold)\n",
    "    predicted_labels_fold = infer_data_labels(X_cluster_labels_fold, cluster_labels_fold)\n",
    "\n",
    "    clusters = kmeans.labels_\n",
    "    label_counts = np.zeros((10, 10))\n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "        label_counts[clusters[i]][y_train[i]] += 1\n",
    "    \n",
    "    print(pd.DataFrame(label_counts, index=row_labels, columns=column_labels))\n",
    "    print()\n",
    "\n",
    "    # if you want to hide all label distribution in clusters, you will comment this section\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            plt.bar(range(10), label_counts[i])\n",
    "            plt.title(f\"Label distribution in cluster {i}\")\n",
    "            plt.xlabel(\"Label\")\n",
    "        plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_train_fold, predicted_labels_fold)\n",
    "    cm = confusion_matrix(y_train_fold, predicted_labels_fold)\n",
    "\n",
    "    accuracy_scores_for_trainin_error.append(accuracy)\n",
    "    confusion_matrices_for_trainin_error.append(cm)"
   ],
   "id": "38a1ec346ce48501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, score in enumerate(accuracy_scores_for_trainin_error):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(confusion_matrices_for_trainin_error[i], annot=True, fmt='d', cmap='Greens')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    i+=1\n",
    "    plt.xlabel(f'{i}. Fold cross validation Score: {score:.3f}')\n",
    "    plt.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate average accuracy across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores_for_trainin_error)\n",
    "print(f\"Average accuracy: {avg_accuracy}\")"
   ],
   "id": "18f02efb97192d50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### IMPORTANT NOTE\n",
    "> As you can be seen from the tables, I created that in the operations performed by applying k fold cross, values in distributions are close to each other. To avoid this, I will try two separate approaches. \n",
    "> First is the approach I'm trying to implement here. The second solution would be to proceed to the result without applying k fold. When I try this, you will see that the distribution looks better."
   ],
   "id": "61e0cf323e0d3408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', 7)\n",
    "\n",
    "row_labels = [f'Label{i}' for i in range(10)]\n",
    "column_labels = [f'Cluster{i+1}' for i in range(10)]\n",
    "\n",
    "accuracy_scores_for_trainin_error = []\n",
    "confusion_matrices_for_trainin_error = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "kmeans = KMeansWithCustomDistance(n_clusters=10, init='k-means++', distance='manhattan')\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "cluster_labels = infer_cluster_labels(kmeans, y_train)\n",
    "X_cluster_labels = kmeans.predict(X_train)\n",
    "predicted_labels = infer_data_labels(X_cluster_labels, cluster_labels)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "label_counts = np.zeros((10, 10))"
   ],
   "id": "685862c38c039a8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(clusters)):\n",
    "    label_counts[clusters[i]][y_train[i]] += 1\n",
    "    \n",
    "conjucted_matrix = (pd.DataFrame(label_counts, index=row_labels, columns=column_labels))"
   ],
   "id": "285558be4cbe6538",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Error â€” Confusion Matrix & Accuracy",
   "id": "eaa4b164ce52d2fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(conjucted_matrix)",
   "id": "fd92f005f2d61eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### All label distribution in clusters",
   "id": "37cf06f59817801e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if you want to see all label distribution in clusters, you will uncomment\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.bar(range(10), label_counts[i])\n",
    "        plt.title(f\"Label distribution in cluster {i}\")\n",
    "        plt.xlabel(\"Label\")\n",
    "    plt.show()"
   ],
   "id": "5075b6da38d25d16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "30d972a816861afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)"
   ],
   "id": "b0a3bbf6dfccc8fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b4264e406a73d91",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
