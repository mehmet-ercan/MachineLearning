{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Some imports",
   "id": "48283fa4698c7758"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.datasets import mnist\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### An existing k-means algorithm with three different distance metrics",
   "id": "5cd01a369c16e191"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
    "\n",
    "class KMeansWithCustomDistance(KMeans):\n",
    "    def __init__(self, n_clusters, init, distance='euclidean', **kwargs):\n",
    "        super().__init__(n_clusters=n_clusters, init=init, **kwargs)\n",
    "        self.distance = distance\n",
    "\n",
    "    def _pairwise_distances_argmin(self, X):\n",
    "        if self.distance == 'euclidean':\n",
    "            return np.argmin(euclidean_distances(X, self.cluster_centers_), axis=1)\n",
    "        elif self.distance == 'manhattan':\n",
    "            return np.argmin(manhattan_distances(X, self.cluster_centers_), axis=1)\n",
    "        elif self.distance == 'cosine':\n",
    "            return np.argmin(1 - cosine_distances(X, self.cluster_centers_), axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid distance metric: {self.distance}\")"
   ],
   "id": "61d9618ff431230e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get MNIST data",
   "id": "ad564f7a2e8c53cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # load_data() function takes 60000 for training and 10000 for test. \n",
    "X_train = X_train.astype('float32') / 255.0 # normalize\n",
    "X_train = X_train.reshape(60000, 28*28) # to dimension reducing with 784=28*28\n",
    "\n",
    "# According to HW4, we need 56000 data, because get %80 training data, %20 test data, so 70.000 * 0.8 = 56.000, \n",
    "X_train = X_train[:5600] # for now, 5600 > %10\n",
    "y_train = y_train[:5600]\n",
    "X_test = X_test[:5600]\n",
    "y_test = y_test[:5600]\n",
    "kmeans = KMeansWithCustomDistance(n_clusters=10, init='k-means++', distance='manhattan')"
   ],
   "id": "f22c1b9f09b75d0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get clusters",
   "id": "abf42a91a4397eb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kmeans.fit(X_train)\n",
    "labels_manhattan = kmeans.predict(X_train)\n",
    "print(\"Cluster labels (Manhattan):\", labels_manhattan)\n",
    "\n",
    "centers_manhattan = kmeans.cluster_centers_\n",
    "print(\"Cluster centers (Manhattan):\", centers_manhattan)\n"
   ],
   "id": "60daca2f0409ad72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### If you see the diagram, you will uncommit",
   "id": "fdce9e209cd741a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# \n",
    "# kmeans.fit(X_train_pca)\n",
    "# labels_manhattan_pca = kmeans.predict(X_train_pca)\n",
    "# \n",
    "# centers_pca_manhattan = kmeans.cluster_centers_\n",
    "# print(centers_pca_manhattan)\n",
    "# \n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=labels_manhattan_pca, cmap='viridis')\n",
    "# plt.scatter(centers_pca_manhattan[:, 0], centers_pca_manhattan[:, 1], c='red', s=100, alpha=0.5)\n",
    "# plt.title('Cluster Centers and Data Points (PCA)')\n",
    "# plt.show()"
   ],
   "id": "71afaf725bd1ea8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## P.S.\n",
    "But these are not real label of each image, since the output of the kmeans.labels_ is just group id for clustering. For example, 6 in kmeans.labels_ has similar features with another 6 in kmeans.labels_. There is no more meaning from the label.\n",
    "\n",
    "To match it with real label, we can tackle the follow things:\n",
    "\n",
    "- Combine each images in the same group\n",
    "- Check Frequency distribution of actual labels (using np.bincount)\n",
    "- Find the Maximum frequent label (through np.argmax), and set the label."
   ],
   "id": "1b40a2395cf12a95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def infer_cluster_labels(kmeans, actual_labels):\n",
    "    \"\"\"\n",
    "    Associates most probable label with each cluster in KMeans model\n",
    "    returns: dictionary of clusters assigned to each label\n",
    "    \"\"\"\n",
    "\n",
    "    inferred_labels = {}\n",
    "\n",
    "    # Loop through the clusters\n",
    "    for i in range(kmeans.n_clusters):\n",
    "\n",
    "        # find index of points in cluster\n",
    "        labels = []\n",
    "        index = np.where(kmeans.labels_ == i)\n",
    "\n",
    "        # append actual labels for each point in cluster\n",
    "        labels.append(actual_labels[index])\n",
    "\n",
    "        # determine most common label\n",
    "        if len(labels[0]) == 1:\n",
    "            counts = np.bincount(labels[0])\n",
    "        else:\n",
    "            counts = np.bincount(np.squeeze(labels))\n",
    "\n",
    "        # assign the cluster to a value in the inferred_labels dictionary\n",
    "        if np.argmax(counts) in inferred_labels:\n",
    "            # append the new number to the existing array at this slot\n",
    "            inferred_labels[np.argmax(counts)].append(i)\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "            inferred_labels[np.argmax(counts)] = [i]\n",
    "        \n",
    "    return inferred_labels  \n",
    "\n",
    "def infer_data_labels(X_labels, cluster_labels):\n",
    "    \"\"\"\n",
    "    Determines label for each array, depending on the cluster it has been assigned to.\n",
    "    returns: predicted labels for each array\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty array of len(X)\n",
    "    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)\n",
    "    \n",
    "    for i, cluster in enumerate(X_labels):\n",
    "        for key, value in cluster_labels.items():\n",
    "            if cluster in value:\n",
    "                predicted_labels[i] = key\n",
    "                \n",
    "    return predicted_labels"
   ],
   "id": "f2a9aad4a140194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kmeans.fit(X_train)\n",
    "cluster_labels = infer_cluster_labels(kmeans, y_train)\n",
    "X_clusters_labels = kmeans.predict(X_train)\n",
    "predicted_labels_with_argmax = infer_data_labels(X_clusters_labels, cluster_labels)\n",
    "\n",
    "print(predicted_labels_with_argmax[:20])\n",
    "print(y_train[:20])"
   ],
   "id": "de291ef210ff8182",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### If you see Kmeans performance for different cluster, you will uncommit",
   "id": "7c95dca26328a9f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# \n",
    "# clusters = [10, 16, 36, 64, 144, 256]\n",
    "# acc_list = []\n",
    "# \n",
    "# for n_clusters in clusters:\n",
    "#     estimator = KMeansWithCustomDistance(n_clusters=n_clusters, init='k-means++', distance='manhattan')\n",
    "#     estimator.fit(X_train)\n",
    "# \n",
    "#     # Determine predicted labels\n",
    "#     cluster_labels = infer_cluster_labels(estimator, y_train)\n",
    "#     prediction = infer_data_labels(estimator.labels_, cluster_labels)\n",
    "#     \n",
    "#     acc = accuracy_score(y_train, prediction)\n",
    "#     acc_list.append(acc)\n",
    "#     print('Accuracy: {}\\n'.format(acc))"
   ],
   "id": "e28ff305ed410b76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', 7)\n",
    "\n",
    "row_labels = [f'Label{i}' for i in range(10)]\n",
    "column_labels = [f'Cluster{i+1}' for i in range(10)]\n",
    "\n",
    "accuracy_scores_for_trainin_error = []\n",
    "confusion_matrices_for_trainin_error = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation this function gives me an index subset of X_train\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    kmeans.fit(X_train_fold)\n",
    "    \n",
    "    cluster_labels_fold = infer_cluster_labels(kmeans, y_train_fold)\n",
    "    X_cluster_labels_fold = kmeans.predict(X_train_fold)\n",
    "    predicted_labels_fold = infer_data_labels(X_cluster_labels_fold, cluster_labels_fold)\n",
    "\n",
    "    clusters = kmeans.labels_\n",
    "    label_counts = np.zeros((10, 10))\n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "        label_counts[clusters[i]][y_train[i]] += 1\n",
    "    \n",
    "    print(pd.DataFrame(label_counts, index=row_labels, columns=column_labels))\n",
    "    print()\n",
    "\n",
    "    # if you want to hide all label distribution in clusters, you will comment this section\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            plt.bar(range(10), label_counts[i])\n",
    "            plt.title(f\"Label distribution in cluster {i}\")\n",
    "            plt.xlabel(\"Label\")\n",
    "        plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_train_fold, predicted_labels_fold)\n",
    "    cm = confusion_matrix(y_train_fold, predicted_labels_fold)\n",
    "\n",
    "    accuracy_scores_for_trainin_error.append(accuracy)\n",
    "    confusion_matrices_for_trainin_error.append(cm)"
   ],
   "id": "38a1ec346ce48501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, score in enumerate(accuracy_scores_for_trainin_error):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(confusion_matrices_for_trainin_error[i], annot=True, fmt='d', cmap='Greens')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    i+=1\n",
    "    plt.xlabel(f'{i}. Fold cross validation Score: {score:.3f}')\n",
    "    plt.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate average accuracy across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores_for_trainin_error)\n",
    "print(f\"Average accuracy: {avg_accuracy}\")"
   ],
   "id": "18f02efb97192d50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# IMPORTANT NOTE\n",
    "> # As you can be seen from the tables, I created that in the operations performed by applying k fold cross, values in distributions are close to each other. To avoid this, I will try two separate approaches. \n",
    "> # First is the approach I'm trying to implement here. The second solution would be to proceed to the result without applying k fold. When I try this, you will see that the distribution looks better."
   ],
   "id": "61e0cf323e0d3408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', 7)\n",
    "\n",
    "row_labels = [f'Label{i}' for i in range(10)]\n",
    "column_labels = [f'Cluster{i+1}' for i in range(10)]\n",
    "\n",
    "accuracy_scores_for_trainin_error = []\n",
    "confusion_matrices_for_trainin_error = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "kmeans = KMeansWithCustomDistance(n_clusters=10, init='k-means++', distance='manhattan')\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "cluster_labels = infer_cluster_labels(kmeans, y_train)\n",
    "X_cluster_labels = kmeans.predict(X_train)\n",
    "predicted_labels = infer_data_labels(X_cluster_labels, cluster_labels)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "label_counts = np.zeros((10, 10))"
   ],
   "id": "685862c38c039a8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:36:27.240807Z",
     "start_time": "2024-05-13T15:36:27.230062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(clusters)):\n",
    "    label_counts[clusters[i]][y_train[i]] += 1\n",
    "    \n",
    "conjucted_matrix = (pd.DataFrame(label_counts, index=row_labels, columns=column_labels))"
   ],
   "id": "285558be4cbe6538",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Error — Confusion Matrix & Accuracy",
   "id": "eaa4b164ce52d2fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:36:29.026421Z",
     "start_time": "2024-05-13T15:36:29.016220Z"
    }
   },
   "cell_type": "code",
   "source": "print(conjucted_matrix)",
   "id": "fd92f005f2d61eb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Cluster1  Cluster2  Cluster3  Cluster4  Cluster5  Cluster6  Cluster7  \\\n",
      "Label0     866.0       0.0       8.0       2.0       0.0      10.0      18.0   \n",
      "Label1       4.0       2.0       4.0      16.0     380.0      76.0       0.0   \n",
      "Label2       0.0     658.0      58.0       2.0      44.0       2.0       4.0   \n",
      "Label3       2.0       0.0       6.0       6.0     248.0      14.0       0.0   \n",
      "Label4       8.0     582.0     138.0     104.0      62.0     268.0     148.0   \n",
      "Label5      46.0       0.0      24.0       8.0      24.0      26.0     832.0   \n",
      "Label6      22.0       2.0      28.0      26.0     412.0      44.0      84.0   \n",
      "Label7      66.0       2.0      76.0     660.0       0.0     268.0       4.0   \n",
      "Label8      12.0       2.0     714.0      34.0       8.0       2.0      14.0   \n",
      "Label9      76.0       2.0      24.0     262.0       0.0     248.0      16.0   \n",
      "\n",
      "        Cluster8  Cluster9  Cluster10  \n",
      "Label0       2.0       0.0        4.0  \n",
      "Label1     464.0      44.0      420.0  \n",
      "Label2      48.0      22.0        6.0  \n",
      "Label3     510.0      18.0      346.0  \n",
      "Label4      60.0     128.0       46.0  \n",
      "Label5       0.0       4.0        4.0  \n",
      "Label6     144.0      46.0      270.0  \n",
      "Label7       0.0     246.0       18.0  \n",
      "Label8       4.0       6.0        0.0  \n",
      "Label9       0.0     530.0        2.0  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### All label distribution in clusters",
   "id": "37cf06f59817801e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# if you want to see all label distribution in clusters, you will uncomment\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.bar(range(10), label_counts[i])\n",
    "        plt.title(f\"Label distribution in cluster {i}\")\n",
    "        plt.xlabel(\"Label\")\n",
    "    plt.show()"
   ],
   "id": "5075b6da38d25d16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "30d972a816861afa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:37:13.323835Z",
     "start_time": "2024-05-13T15:37:13.282255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)"
   ],
   "id": "b0a3bbf6dfccc8fb",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. KNeighborsClassifier expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m knn \u001B[38;5;241m=\u001B[39m KNeighborsClassifier(n_neighbors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      2\u001B[0m knn\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m----> 3\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m knn\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:234\u001B[0m, in \u001B[0;36mKNeighborsClassifier.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;124;03m    Class labels for each data sample.\u001B[39;00m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# In that case, we do not need the distances to perform\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# the weighting so we do not compute them.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     neigh_ind \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkneighbors(X, return_distance\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    235\u001B[0m     neigh_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:806\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[0;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[1;32m    804\u001B[0m         X \u001B[38;5;241m=\u001B[39m _check_precomputed(X)\n\u001B[1;32m    805\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 806\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(X, accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    808\u001B[0m n_samples_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_fit_\n\u001B[1;32m    809\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_neighbors \u001B[38;5;241m>\u001B[39m n_samples_fit:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:565\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    563\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 565\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[1;32m    566\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:915\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    910\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    912\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    913\u001B[0m     )\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nd \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m--> 915\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    916\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    917\u001B[0m         \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m    918\u001B[0m     )\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m    921\u001B[0m     _assert_all_finite(\n\u001B[1;32m    922\u001B[0m         array,\n\u001B[1;32m    923\u001B[0m         input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[1;32m    924\u001B[0m         estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[1;32m    925\u001B[0m         allow_nan\u001B[38;5;241m=\u001B[39mforce_all_finite \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    926\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found array with dim 3. KNeighborsClassifier expected <= 2."
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b4264e406a73d91",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
